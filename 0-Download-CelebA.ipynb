{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.18.4-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |################################| 92kB 2.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.17.1-py2.py3-none-any.whl (47kB)\n",
      "\u001b[K    100% |################################| 51kB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |################################| 143kB 2.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2017.7.27.1-py2.py3-none-any.whl (349kB)\n",
      "\u001b[K    100% |################################| 358kB 1.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.7,>=2.5 (from requests)\n",
      "  Downloading idna-2.6-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |################################| 61kB 5.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting urllib3<1.23,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n",
      "\u001b[K    100% |################################| 133kB 2.6MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: chardet, certifi, idna, urllib3, requests, tqdm\n",
      "Successfully installed certifi-2017.7.27.1 chardet-3.0.4 idna-2.6 requests-2.18.4 tqdm-4.17.1 urllib3-1.22\n"
     ]
    }
   ],
   "source": [
    "!pip install requests tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modification of https://github.com/stanfordnlp/treelstm/blob/master/scripts/download.py\n",
    "\n",
    "Downloads the following:\n",
    "- Celeb-A dataset\n",
    "- LSUN dataset\n",
    "- MNIST dataset\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "import requests\n",
    "from six.moves import urllib\n",
    "from tqdm import tqdm\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Download dataset for DCGAN.')\n",
    "parser.add_argument('datasets', metavar='N', type=str, nargs='+', choices=['celebA', 'lsun', 'mnist'],\n",
    "                    help='name of dataset to download [celebA, lsun, mnist]')\n",
    "\n",
    "def download(url, dirpath):\n",
    "    filename = url.split('/')[-1]\n",
    "    filepath = os.path.join(dirpath, filename)\n",
    "    u = urllib.request.urlopen(url)\n",
    "    f = open(filepath, 'wb')\n",
    "    filesize = int(u.headers[\"Content-Length\"])\n",
    "    print(\"Downloading: %s Bytes: %s\" % (filename, filesize))\n",
    "\n",
    "    downloaded = 0\n",
    "    block_sz = 8192\n",
    "    status_width = 70\n",
    "    while True:\n",
    "        buf = u.read(block_sz)\n",
    "        if not buf:\n",
    "            print('')\n",
    "            break\n",
    "        else:\n",
    "            print('', end='\\r')\n",
    "        downloaded += len(buf)\n",
    "        f.write(buf)\n",
    "        status = ((\"[%-\" + str(status_width + 1) + \"s] %3.2f%%\") %\n",
    "                  ('=' * int(float(downloaded) / filesize * status_width) + '>', downloaded * 100. / filesize))\n",
    "        print(status, end='')\n",
    "        sys.stdout.flush()\n",
    "    f.close()\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params={'id': id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = {'id': id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    save_response_content(response, destination)\n",
    "\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_response_content(response, destination, chunk_size=32 * 1024):\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in tqdm(response.iter_content(chunk_size), total=total_size,\n",
    "                          unit='B', unit_scale=True, desc=destination):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "def unzip(filepath):\n",
    "    print(\"Extracting: \" + filepath)\n",
    "    dirpath = os.path.dirname(filepath)\n",
    "    with zipfile.ZipFile(filepath) as zf:\n",
    "        zf.extractall(dirpath)\n",
    "    os.remove(filepath)\n",
    "\n",
    "\n",
    "def download_celeb_a(dirpath):\n",
    "    data_dir = 'celebA'\n",
    "    if os.path.exists(os.path.join(dirpath, data_dir)):\n",
    "        print('Found Celeb-A - skip')\n",
    "        return\n",
    "\n",
    "    filename, drive_id = \"img_align_celeba.zip\", \"0B7EVK8r0v71pZjFTYXZWM3FlRnM\"\n",
    "    save_path = os.path.join(dirpath, filename)\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print('[*] {} already exists'.format(save_path))\n",
    "    else:\n",
    "        download_file_from_google_drive(drive_id, save_path)\n",
    "\n",
    "    zip_dir = ''\n",
    "    with zipfile.ZipFile(save_path) as zf:\n",
    "        zip_dir = zf.namelist()[0]\n",
    "        zf.extractall(dirpath)\n",
    "    os.remove(save_path)\n",
    "    os.rename(os.path.join(dirpath, zip_dir), os.path.join(dirpath, data_dir))\n",
    "\n",
    "\n",
    "def _list_categories(tag):\n",
    "    url = 'http://lsun.cs.princeton.edu/htbin/list.cgi?tag=' + tag\n",
    "    f = urllib.request.urlopen(url)\n",
    "    return json.loads(f.read())\n",
    "\n",
    "\n",
    "def _download_lsun(out_dir, category, set_name, tag):\n",
    "    url = 'http://lsun.cs.princeton.edu/htbin/download.cgi?tag={tag}' \\\n",
    "          '&category={category}&set={set_name}'.format(**locals())\n",
    "    print(url)\n",
    "    if set_name == 'test':\n",
    "        out_name = 'test_lmdb.zip'\n",
    "    else:\n",
    "        out_name = '{category}_{set_name}_lmdb.zip'.format(**locals())\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    cmd = ['curl', url, '-o', out_path]\n",
    "    print('Downloading', category, set_name, 'set')\n",
    "    subprocess.call(cmd)\n",
    "\n",
    "\n",
    "def download_lsun(dirpath):\n",
    "    data_dir = os.path.join(dirpath, 'lsun')\n",
    "    if os.path.exists(data_dir):\n",
    "        print('Found LSUN - skip')\n",
    "        return\n",
    "    else:\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    tag = 'latest'\n",
    "    categories = ['bedroom']\n",
    "\n",
    "    for category in categories:\n",
    "        _download_lsun(data_dir, category, 'train', tag)\n",
    "        _download_lsun(data_dir, category, 'val', tag)\n",
    "    _download_lsun(data_dir, '', 'test', tag)\n",
    "\n",
    "\n",
    "def download_mnist(dirpath):\n",
    "    data_dir = os.path.join(dirpath, 'mnist')\n",
    "    if os.path.exists(data_dir):\n",
    "        print('Found MNIST - skip')\n",
    "        return\n",
    "    else:\n",
    "        os.mkdir(data_dir)\n",
    "    url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    file_names = ['train-images-idx3-ubyte.gz',\n",
    "                  'train-labels-idx1-ubyte.gz',\n",
    "                  't10k-images-idx3-ubyte.gz',\n",
    "                  't10k-labels-idx1-ubyte.gz']\n",
    "    for file_name in file_names:\n",
    "        url = (url_base + file_name).format(**locals())\n",
    "        print(url)\n",
    "        out_path = os.path.join(data_dir, file_name)\n",
    "        cmd = ['curl', url, '-o', out_path]\n",
    "        print('Downloading ', file_name)\n",
    "        subprocess.call(cmd)\n",
    "        cmd = ['gzip', '-d', out_path]\n",
    "        print('Decompressing ', file_name)\n",
    "        subprocess.call(cmd)\n",
    "\n",
    "\n",
    "def prepare_data_dir(path='./data'):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and splitting celeb-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./datasets/img_align_celeba.zip: 44.1KB [03:50, 191B/s] \n"
     ]
    }
   ],
   "source": [
    "download_celeb_a('./datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_link(in_dir, basename, out_dir):\n",
    "    in_file = os.path.join(in_dir, basename)\n",
    "    if os.path.exists(in_file):\n",
    "        link_file = os.path.join(out_dir, basename)\n",
    "        rel_link = os.path.relpath(in_file, out_dir)\n",
    "        os.symlink(rel_link, link_file)\n",
    "\n",
    "\n",
    "def add_celeb_splits():\n",
    "    data_path = os.path.join('./datasets/')\n",
    "    images_path = os.path.join(data_path, 'celebA')\n",
    "    train_dir = os.path.join(data_path, 'celeb-splitted', 'splits', 'train', 'images')\n",
    "    valid_dir = os.path.join(data_path, 'celeb-splitted', 'splits', 'valid', 'images')\n",
    "    test_dir = os.path.join(data_path, 'celeb-splitted', 'splits', 'test', 'images')\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    if not os.path.exists(valid_dir):\n",
    "        os.makedirs(valid_dir)\n",
    "    if not os.path.exists(test_dir):\n",
    "        os.makedirs(test_dir)\n",
    "\n",
    "    # these constants based on the standard CelebA splits\n",
    "    NUM_EXAMPLES = 202599\n",
    "    TRAIN_STOP = 162770\n",
    "    VALID_STOP = 182637\n",
    "\n",
    "    for i in range(0, TRAIN_STOP):\n",
    "        basename = \"{:06d}.jpg\".format(i+1)\n",
    "        check_link(images_path, basename, train_dir)\n",
    "    for i in range(TRAIN_STOP, VALID_STOP):\n",
    "        basename = \"{:06d}.jpg\".format(i+1)\n",
    "        check_link(images_path, basename, valid_dir)\n",
    "    for i in range(VALID_STOP, NUM_EXAMPLES):\n",
    "        basename = \"{:06d}.jpg\".format(i+1)\n",
    "        check_link(images_path, basename, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_celeb_splits()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
